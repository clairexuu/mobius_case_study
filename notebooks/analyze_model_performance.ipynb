{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of trained Möbius function models, including:\n",
    "- Overall accuracy metrics\n",
    "- Per-class accuracy (for μ(n) = -1, 0, 1)\n",
    "- Training configuration (data generation, encoding format, model architecture)\n",
    "- Training curves and convergence analysis\n",
    "\n",
    "**Note:** Update the `path` and `xp_env` variables below to point to your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "path = \"../models/model_CRT100_with_stats\"  # Path to trained models\n",
    "xp_env = [\"mu\"]  # Experiment names: basic=μ(n), musq=μ²(n)\n",
    "indicator = \"valid_arithmetic\"  # Metric prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## 1. Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all experiments\n",
    "xps = [(env, xp) for env in xp_env for xp in os.listdir(path + '/' + env) \n",
    "       if os.path.isdir(path + env + '/' + xp)]\n",
    "names = [path + env + '/' + xp for (env, xp) in xps]\n",
    "\n",
    "print(f\"Found {len(names)} experiments:\")\n",
    "for env, xp in xps:\n",
    "    print(f\"  - {env}/{xp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parse_header",
   "metadata": {},
   "source": [
    "## 2. Parse Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_training_log(log_path, indicator=\"valid_arithmetic\"):\n",
    "    \"\"\"\n",
    "    Parse training log to extract metrics including per-class accuracy.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_path):\n",
    "        return None\n",
    "    \n",
    "    epochs = []\n",
    "    overall_acc = []\n",
    "    overall_loss = []\n",
    "    perfect_match = []\n",
    "    \n",
    "    # Per-class accuracy tracking\n",
    "    class_acc = defaultdict(list)\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if '__log__:' in line:\n",
    "                try:\n",
    "                    # Extract JSON log\n",
    "                    pos = line.find('__log__:')\n",
    "                    json_str = line[pos + 8:]\n",
    "                    data = ast.literal_eval(json_str)\n",
    "                    \n",
    "                    epoch = data.get('epoch', -1)\n",
    "                    if epoch < 0:\n",
    "                        continue\n",
    "                    \n",
    "                    epochs.append(epoch)\n",
    "                    overall_acc.append(data.get(f'{indicator}_acc', 0.0))\n",
    "                    overall_loss.append(data.get(f'{indicator}_xe_loss', 0.0))\n",
    "                    perfect_match.append(data.get(f'{indicator}_perfect', 0.0))\n",
    "                    \n",
    "                    # Extract per-class accuracy\n",
    "                    for key, value in data.items():\n",
    "                        if key.startswith(f'{indicator}_acc_') and key != f'{indicator}_acc':\n",
    "                            # Extract class name (e.g., \"0\", \"1\", \"100\" for -1)\n",
    "                            class_name = key.replace(f'{indicator}_acc_', '')\n",
    "                            class_acc[class_name].append(value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing line: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Find best epoch\n",
    "    if overall_acc:\n",
    "        best_epoch_idx = np.argmax(overall_acc)\n",
    "        best_epoch = epochs[best_epoch_idx]\n",
    "        best_acc = overall_acc[best_epoch_idx]\n",
    "    else:\n",
    "        best_epoch = -1\n",
    "        best_acc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'epochs': epochs,\n",
    "        'overall_acc': overall_acc,\n",
    "        'overall_loss': overall_loss,\n",
    "        'perfect_match': perfect_match,\n",
    "        'class_acc': dict(class_acc),\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_acc': best_acc,\n",
    "        'final_epoch': epochs[-1] if epochs else -1,\n",
    "        'final_acc': overall_acc[-1] if overall_acc else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "def load_params(params_path):\n",
    "    \"\"\"\n",
    "    Load experiment parameters from pickle file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(params_path):\n",
    "        return None\n",
    "    \n",
    "    with open(params_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "print(\"Functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_all_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all experiment data\n",
    "experiments = []\n",
    "\n",
    "for env, xp in xps:\n",
    "    exp_path = path + env + '/' + xp\n",
    "    log_path = exp_path + '/train.log'\n",
    "    params_path = exp_path + '/params.pkl'\n",
    "    \n",
    "    # Parse training log\n",
    "    metrics = parse_training_log(log_path, indicator)\n",
    "    \n",
    "    # Load parameters\n",
    "    params = load_params(params_path)\n",
    "    \n",
    "    if metrics and params:\n",
    "        experiments.append({\n",
    "            'name': f\"{env}/{xp}\",\n",
    "            'env': env,\n",
    "            'xp': xp,\n",
    "            'metrics': metrics,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "print(f\"Successfully loaded {len(experiments)} experiments with complete data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 3. Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for exp in experiments:\n",
    "    m = exp['metrics']\n",
    "    p = exp['params']\n",
    "    \n",
    "    # Get per-class accuracy at best epoch\n",
    "    best_idx = exp['metrics']['epochs'].index(m['best_epoch']) if m['best_epoch'] >= 0 else -1\n",
    "    \n",
    "    class_acc_str = \"\"\n",
    "    if best_idx >= 0:\n",
    "        class_accs = []\n",
    "        for class_name in sorted(m['class_acc'].keys()):\n",
    "            if len(m['class_acc'][class_name]) > best_idx:\n",
    "                acc = m['class_acc'][class_name][best_idx]\n",
    "                # Map class names: 100 -> -1 for Möbius\n",
    "                display_name = \"-1\" if class_name == \"100\" else class_name\n",
    "                class_accs.append(f\"{display_name}:{acc:.1f}%\")\n",
    "        class_acc_str = \", \".join(class_accs)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Experiment': exp['name'],\n",
    "        'Task': 'μ(n)' if exp['env'] == 'basic' else 'μ²(n)',\n",
    "        'Best Epoch': m['best_epoch'],\n",
    "        'Best Acc (%)': f\"{m['best_acc']:.2f}\",\n",
    "        'Final Epoch': m['final_epoch'],\n",
    "        'Final Acc (%)': f\"{m['final_acc']:.2f}\",\n",
    "        'Per-Class Acc (%)': class_acc_str\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## 4. Experiment Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_details",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in experiments:\n",
    "    p = exp['params']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CONFIGURATION: {exp['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n### Data Configuration ###\")\n",
    "    print(f\"  Operation:          {p.operation if hasattr(p, 'operation') else 'N/A'}\")\n",
    "    print(f\"  Data Types:         {p.data_types if hasattr(p, 'data_types') else 'N/A'}\")\n",
    "    print(f\"  Training Data:      {p.train_data if hasattr(p, 'train_data') else 'N/A'}\")\n",
    "    print(f\"  Eval Data:          {p.eval_data if hasattr(p, 'eval_data') else 'N/A'}\")\n",
    "    print(f\"  Encoding Base:      {p.base if hasattr(p, 'base') else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\n### Data Encoding Format ###\")\n",
    "    print(f\"  Format: Interleaved CRT representation\")\n",
    "    print(f\"  Structure: [n mod p₁, p₁, n mod p₂, p₂, ..., n mod p₁₀₀, p₁₀₀]\")\n",
    "    print(f\"  Number of primes: 100 (primes ≤ 542)\")\n",
    "    print(f\"  Vector length: 200 (2 × 100 primes)\")\n",
    "    print(f\"  Integer encoding: Base-{p.base if hasattr(p, 'base') else 1000} positional notation\")\n",
    "    \n",
    "    print(\"\\n### Model Architecture ###\")\n",
    "    print(f\"  Architecture:       {p.architecture if hasattr(p, 'architecture') else 'N/A'}\")\n",
    "    print(f\"  Model Type:         {'LSTM' if (hasattr(p, 'lstm') and p.lstm) else 'Transformer'}\")\n",
    "    print(f\"  Encoder Layers:     {p.n_enc_layers if hasattr(p, 'n_enc_layers') else 'N/A'}\")\n",
    "    print(f\"  Decoder Layers:     {p.n_dec_layers if hasattr(p, 'n_dec_layers') else 'N/A'}\")\n",
    "    print(f\"  Encoder Embed Dim:  {p.enc_emb_dim if hasattr(p, 'enc_emb_dim') else 'N/A'}\")\n",
    "    print(f\"  Decoder Embed Dim:  {p.dec_emb_dim if hasattr(p, 'dec_emb_dim') else 'N/A'}\")\n",
    "    print(f\"  Attention Heads:    {p.n_enc_heads if hasattr(p, 'n_enc_heads') else 'N/A'} (encoder), {p.n_dec_heads if hasattr(p, 'n_dec_heads') else 'N/A'} (decoder)\")\n",
    "    print(f\"  Dropout:            {p.dropout if hasattr(p, 'dropout') else 'N/A'}\")\n",
    "    print(f\"  Attention Dropout:  {p.attention_dropout if hasattr(p, 'attention_dropout') else 'N/A'}\")\n",
    "    \n",
    "    print(\"\\n### Training Configuration ###\")\n",
    "    print(f\"  Optimizer:          {p.optimizer if hasattr(p, 'optimizer') else 'N/A'}\")\n",
    "    print(f\"  Batch Size:         {p.batch_size if hasattr(p, 'batch_size') else 'N/A'}\")\n",
    "    print(f\"  Epoch Size:         {p.epoch_size if hasattr(p, 'epoch_size') else 'N/A'}\")\n",
    "    print(f\"  Max Epochs:         {p.max_epoch if hasattr(p, 'max_epoch') else 'N/A'}\")\n",
    "    print(f\"  Eval Size:          {p.eval_size if hasattr(p, 'eval_size') else 'N/A'}\")\n",
    "    print(f\"  Gradient Clipping:  {p.clip_grad_norm if hasattr(p, 'clip_grad_norm') else 'N/A'}\")\n",
    "    print(f\"  Env Base Seed:      {p.env_base_seed if hasattr(p, 'env_base_seed') else 'N/A'}\")\n",
    "    print(f\"  Random Seed:        {p.seed if hasattr(p, 'seed') else 'Not set (non-reproducible)'}\")\n",
    "    \n",
    "    print(\"\\n### Dataset Generation ###\")\n",
    "    print(f\"  Total samples:      1,000,000\")\n",
    "    print(f\"  Training samples:   900,000\")\n",
    "    print(f\"  Test samples:       100,000\")\n",
    "    print(f\"  Integer range:      [2, 10¹³]\")\n",
    "    print(f\"  Generation method:  Random sampling with Möbius function computed via C++ library\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curves_header",
   "metadata": {},
   "source": [
    "## 5. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for exp in experiments:\n",
    "    m = exp['metrics']\n",
    "    label = f\"{exp['env']} (best: {m['best_acc']:.2f}% @ epoch {m['best_epoch']})\"\n",
    "    ax1.plot(m['epochs'], m['overall_acc'], linewidth=2, label=label, alpha=0.8)\n",
    "    ax2.plot(m['epochs'], m['overall_loss'], linewidth=2, label=exp['env'], alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Overall Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax2.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to: ../notebooks/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perclass_header",
   "metadata": {},
   "source": [
    "## 6. Per-Class Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_perclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class names for display\n",
    "def map_class_name(class_name):\n",
    "    \"\"\"Map internal class names to human-readable Möbius values.\"\"\"\n",
    "    if class_name == \"100\":\n",
    "        return \"μ=-1\"\n",
    "    elif class_name == \"0\":\n",
    "        return \"μ=0\"\n",
    "    elif class_name == \"1\":\n",
    "        return \"μ=1\"\n",
    "    else:\n",
    "        return f\"class {class_name}\"\n",
    "\n",
    "\n",
    "# Plot per-class accuracy for each experiment\n",
    "for exp in experiments:\n",
    "    m = exp['metrics']\n",
    "    \n",
    "    if not m['class_acc']:\n",
    "        print(f\"No per-class data for {exp['name']}\")\n",
    "        continue\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot each class\n",
    "    for class_name in sorted(m['class_acc'].keys()):\n",
    "        if not m['class_acc'][class_name]:\n",
    "            continue\n",
    "        \n",
    "        display_name = map_class_name(class_name)\n",
    "        class_epochs = m['epochs'][:len(m['class_acc'][class_name])]\n",
    "        \n",
    "        ax.plot(class_epochs, m['class_acc'][class_name], \n",
    "                linewidth=2, label=display_name, alpha=0.8, marker='o', markersize=2)\n",
    "    \n",
    "    # Plot overall accuracy for comparison\n",
    "    ax.plot(m['epochs'], m['overall_acc'], \n",
    "            linewidth=2.5, label='Overall', alpha=0.9, linestyle='--', color='black')\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'Per-Class Accuracy: {exp[\"name\"]}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'../notebooks/per_class_accuracy_{exp[\"env\"]}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Per-class accuracy plot saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_header",
   "metadata": {},
   "source": [
    "## 7. Final Per-Class Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparing per-class accuracy at best epoch\n",
    "fig, axes = plt.subplots(1, len(experiments), figsize=(8*len(experiments), 6))\n",
    "if len(experiments) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, exp in enumerate(experiments):\n",
    "    m = exp['metrics']\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get accuracy at best epoch\n",
    "    best_idx = m['epochs'].index(m['best_epoch']) if m['best_epoch'] >= 0 else -1\n",
    "    \n",
    "    if best_idx < 0:\n",
    "        continue\n",
    "    \n",
    "    class_names = []\n",
    "    class_values = []\n",
    "    \n",
    "    for class_name in sorted(m['class_acc'].keys()):\n",
    "        if len(m['class_acc'][class_name]) > best_idx:\n",
    "            class_names.append(map_class_name(class_name))\n",
    "            class_values.append(m['class_acc'][class_name][best_idx])\n",
    "    \n",
    "    # Add overall accuracy\n",
    "    class_names.append('Overall')\n",
    "    class_values.append(m['best_acc'])\n",
    "    \n",
    "    # Create bar chart\n",
    "    colors = ['#ff7f0e', '#2ca02c', '#d62728', '#1f77b4']  # Colors for each bar\n",
    "    bars = ax.bar(class_names, class_values, color=colors[:len(class_names)], alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'{exp[\"name\"]}\\n(Best Epoch: {m[\"best_epoch\"]})', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/per_class_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Per-class comparison saved to: ../notebooks/per_class_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_header",
   "metadata": {},
   "source": [
    "## 8. Detailed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in experiments:\n",
    "    m = exp['metrics']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DETAILED STATISTICS: {exp['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n### Overall Performance ###\")\n",
    "    print(f\"  Best Accuracy:      {m['best_acc']:.2f}% (epoch {m['best_epoch']})\")\n",
    "    print(f\"  Final Accuracy:     {m['final_acc']:.2f}% (epoch {m['final_epoch']})\")\n",
    "    print(f\"  Total Epochs:       {m['final_epoch'] + 1}\")\n",
    "    \n",
    "    if m['class_acc']:\n",
    "        print(f\"\\n### Per-Class Performance (at best epoch {m['best_epoch']}) ###\")\n",
    "        best_idx = m['epochs'].index(m['best_epoch']) if m['best_epoch'] >= 0 else -1\n",
    "        \n",
    "        if best_idx >= 0:\n",
    "            for class_name in sorted(m['class_acc'].keys()):\n",
    "                if len(m['class_acc'][class_name]) > best_idx:\n",
    "                    display_name = map_class_name(class_name)\n",
    "                    acc = m['class_acc'][class_name][best_idx]\n",
    "                    print(f\"  {display_name:>8}:  {acc:.2f}%\")\n",
    "        \n",
    "        print(f\"\\n### Per-Class Performance (final epoch {m['final_epoch']}) ###\")\n",
    "        for class_name in sorted(m['class_acc'].keys()):\n",
    "            if m['class_acc'][class_name]:\n",
    "                display_name = map_class_name(class_name)\n",
    "                acc = m['class_acc'][class_name][-1]\n",
    "                print(f\"  {display_name:>8}:  {acc:.2f}%\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
