{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on Multiple Datasets\n",
    "\n",
    "This notebook evaluates a trained model on three test datasets:\n",
    "- **Natural**: Uniformly random samples from [1, 10^13]\n",
    "- **Cheat**: Numbers with prime factors only within the first 100 primes\n",
    "- **Non-cheat**: Numbers with at least one prime factor outside the first 100 primes\n",
    "\n",
    "It reports per-class performance for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Specify the model checkpoint and encoding to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "ENCODING = \"CRT100_with_stats\"  # Options: interCRT100, CRT100, interCRT100_with_n, CRT100_with_stats\n",
    "TASK = \"mu\"  # Options: mu, musq\n",
    "MODEL_DIR = \"models/train_CRT100stat\"  # Path to the directory containing the trained model\n",
    "CHECKPOINT_NAME = \"best-valid_arithmetic_acc\"  # Name of checkpoint file (without .pth)\n",
    "\n",
    "# Test datasets directory (will be created if needed)\n",
    "INPUT_BASE_DIR = Path(\"input\")\n",
    "\n",
    "# Results output directory\n",
    "RESULTS_DIR = Path(\"test_results\") / f\"{ENCODING}_{TASK}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Test dataset types\n",
    "DATASET_TYPES = [\"natural\", \"cheat\", \"non_cheat\"]\n",
    "\n",
    "# Int2Int directory\n",
    "INT2INT_DIR = Path(\"Int2Int\")\n",
    "\n",
    "print(f\"Testing model: {MODEL_DIR}/{CHECKPOINT_NAME}.pth\")\n",
    "print(f\"Encoding: {ENCODING}\")\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_params(encoding):\n",
    "    \"\"\"\n",
    "    Get data_types parameter for each encoding.\n",
    "    \"\"\"\n",
    "    encoding_map = {\n",
    "        \"interCRT100\": \"int[200]:range(-1,2)\",\n",
    "        \"CRT100\": \"int[100]:range(-1,2)\",\n",
    "        \"interCRT100_with_n\": \"int[201]:range(-1,2)\",\n",
    "        \"CRT100_with_stats\": \"int[103]:range(-1,2)\"\n",
    "    }\n",
    "    return encoding_map.get(encoding, \"int[200]:range(-1,2)\")\n",
    "\n",
    "\n",
    "def get_test_data_path(encoding, dataset_type, task):\n",
    "    \"\"\"\n",
    "    Construct the path to test data file.\n",
    "    \"\"\"\n",
    "    if task == \"mu\":\n",
    "        base_name = f\"mu_{encoding}\"\n",
    "    elif task == \"musq\":\n",
    "        base_name = f\"musq_{encoding}\"\n",
    "    else:\n",
    "        base_name = f\"{task}_{encoding}\"\n",
    "    \n",
    "    # Construct directory name\n",
    "    input_dir = INPUT_BASE_DIR / f\"input_dir_{encoding}_{dataset_type}\"\n",
    "    test_file = input_dir / f\"{base_name}_{dataset_type}.txt.test\"\n",
    "    \n",
    "    return test_file\n",
    "\n",
    "\n",
    "def parse_log_output(log_text):\n",
    "    \"\"\"\n",
    "    Parse the evaluation output to extract metrics.\n",
    "    \"\"\"\n",
    "    # Find the __log__ JSON line\n",
    "    log_match = re.search(r'__log__:({.+})', log_text)\n",
    "    if log_match:\n",
    "        try:\n",
    "            return json.loads(log_match.group(1))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def interpret_class_id(class_id, task):\n",
    "    \"\"\"\n",
    "    Interpret class ID based on task type.\n",
    "    \"\"\"\n",
    "    if task == 'mu':\n",
    "        if class_id == '0':\n",
    "            return 'μ(n) = 0'\n",
    "        elif class_id == '1':\n",
    "            return 'μ(n) = 1'\n",
    "        elif class_id == '100':\n",
    "            return 'μ(n) = -1'\n",
    "        else:\n",
    "            return f'Class {class_id}'\n",
    "    elif task == 'musq':\n",
    "        if class_id == '0':\n",
    "            return 'μ²(n) = 0'\n",
    "        elif class_id == '1':\n",
    "            return 'μ²(n) = 1'\n",
    "        else:\n",
    "            return f'Class {class_id}'\n",
    "    else:\n",
    "        return f'Class {class_id}'\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if Model Checkpoint Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(MODEL_DIR) / f\"{CHECKPOINT_NAME}.pth\"\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"✓ Checkpoint found: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"✗ Checkpoint NOT found: {checkpoint_path}\")\n",
    "    print(f\"\\nAvailable files in {MODEL_DIR}:\")\n",
    "    model_dir_path = Path(MODEL_DIR)\n",
    "    if model_dir_path.exists():\n",
    "        for f in model_dir_path.iterdir():\n",
    "            print(f\"  - {f.name}\")\n",
    "    else:\n",
    "        print(f\"  Directory {MODEL_DIR} does not exist!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NOTE: If no checkpoint exists, you need to:\")\n",
    "    print(\"1. Train a model with --save_periodic flag, OR\")\n",
    "    print(\"2. The model will auto-save 'best-valid_arithmetic_acc.pth' during training\")\n",
    "    print(\"3. Check the train.log to see if 'Saving best' messages appear\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Test Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking test data files:\\n\")\n",
    "test_files = {}\n",
    "\n",
    "for dataset_type in DATASET_TYPES:\n",
    "    test_file = get_test_data_path(ENCODING, dataset_type, TASK)\n",
    "    test_files[dataset_type] = test_file\n",
    "    \n",
    "    if test_file.exists():\n",
    "        file_size = test_file.stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"✓ {dataset_type:12s}: {test_file} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"✗ {dataset_type:12s}: {test_file} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTE: If test files don't exist, generate them using:\")\n",
    "print(f\"  make -C src/run_int2int_scripts data_all ENCODING={ENCODING}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on Each Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model_dir, checkpoint_name, test_data_path, encoding, task, output_file):\n",
    "    \"\"\"\n",
    "    Run evaluation using the Int2Int training script with --eval_only flag.\n",
    "    \"\"\"\n",
    "    data_types = get_encoding_params(encoding)\n",
    "    checkpoint_path = Path(model_dir) / f\"{checkpoint_name}.pth\"\n",
    "    \n",
    "    # Construct the command\n",
    "    cmd = [\n",
    "        \"python\", str(INT2INT_DIR / \"train.py\"),\n",
    "        \"--eval_only\", \"True\",\n",
    "        \"--eval_from_exp\", str(Path(model_dir).absolute()),\n",
    "        \"--reload_checkpoint\", str(checkpoint_path.absolute()),\n",
    "        \"--eval_data\", str(test_data_path.absolute()),\n",
    "        \"--eval_size\", \"10000\",\n",
    "        \"--data_types\", data_types,\n",
    "        \"--operation\", \"data\",\n",
    "        \"--cpu\", \"True\",  # Run on CPU for notebook compatibility\n",
    "        \"--num_workers\", \"0\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nRunning command:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Run the command\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        # Save full output to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"STDOUT:\\n\")\n",
    "            f.write(result.stdout)\n",
    "            f.write(\"\\n\\nSTDERR:\\n\")\n",
    "            f.write(result.stderr)\n",
    "        \n",
    "        print(f\"Full output saved to: {output_file}\")\n",
    "        \n",
    "        # Parse metrics from output\n",
    "        metrics = parse_log_output(result.stdout)\n",
    "        if metrics is None:\n",
    "            metrics = parse_log_output(result.stderr)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"\\n⚠ Warning: Command exited with code {result.returncode}\")\n",
    "            print(\"Last 50 lines of stderr:\")\n",
    "            print(\"\\n\".join(result.stderr.split(\"\\n\")[-50:]))\n",
    "        \n",
    "        return metrics, result\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"\\n✗ Evaluation timed out after 10 minutes\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error running evaluation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"Evaluation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Natural Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"natural\"\n",
    "test_file = test_files[dataset_type]\n",
    "output_file = RESULTS_DIR / f\"eval_{dataset_type}.log\"\n",
    "\n",
    "if not test_file.exists():\n",
    "    print(f\"✗ Test file not found: {test_file}\")\n",
    "    natural_metrics = None\n",
    "elif not checkpoint_path.exists():\n",
    "    print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "    natural_metrics = None\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING ON NATURAL DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "    natural_metrics, result = run_evaluation(\n",
    "        MODEL_DIR, CHECKPOINT_NAME, test_file, ENCODING, TASK, output_file\n",
    "    )\n",
    "    \n",
    "    if natural_metrics:\n",
    "        print(\"\\n✓ Evaluation completed successfully!\")\n",
    "        print(f\"\\nOverall Accuracy: {natural_metrics.get('valid_arithmetic_acc', 'N/A'):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✗ Failed to parse metrics from evaluation output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Cheat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"cheat\"\n",
    "test_file = test_files[dataset_type]\n",
    "output_file = RESULTS_DIR / f\"eval_{dataset_type}.log\"\n",
    "\n",
    "if not test_file.exists():\n",
    "    print(f\"✗ Test file not found: {test_file}\")\n",
    "    cheat_metrics = None\n",
    "elif not checkpoint_path.exists():\n",
    "    print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "    cheat_metrics = None\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING ON CHEAT DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "    cheat_metrics, result = run_evaluation(\n",
    "        MODEL_DIR, CHECKPOINT_NAME, test_file, ENCODING, TASK, output_file\n",
    "    )\n",
    "    \n",
    "    if cheat_metrics:\n",
    "        print(\"\\n✓ Evaluation completed successfully!\")\n",
    "        print(f\"\\nOverall Accuracy: {cheat_metrics.get('valid_arithmetic_acc', 'N/A'):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✗ Failed to parse metrics from evaluation output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Non-Cheat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"non_cheat\"\n",
    "test_file = test_files[dataset_type]\n",
    "output_file = RESULTS_DIR / f\"eval_{dataset_type}.log\"\n",
    "\n",
    "if not test_file.exists():\n",
    "    print(f\"✗ Test file not found: {test_file}\")\n",
    "    non_cheat_metrics = None\n",
    "elif not checkpoint_path.exists():\n",
    "    print(f\"✗ Checkpoint not found: {checkpoint_path}\")\n",
    "    non_cheat_metrics = None\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATING ON NON-CHEAT DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "    non_cheat_metrics, result = run_evaluation(\n",
    "        MODEL_DIR, CHECKPOINT_NAME, test_file, ENCODING, TASK, output_file\n",
    "    )\n",
    "    \n",
    "    if non_cheat_metrics:\n",
    "        print(\"\\n✓ Evaluation completed successfully!\")\n",
    "        print(f\"\\nOverall Accuracy: {non_cheat_metrics.get('valid_arithmetic_acc', 'N/A'):.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✗ Failed to parse metrics from evaluation output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all metrics\n",
    "all_metrics = {\n",
    "    'natural': natural_metrics,\n",
    "    'cheat': cheat_metrics,\n",
    "    'non_cheat': non_cheat_metrics\n",
    "}\n",
    "\n",
    "# Filter out None values\n",
    "all_metrics = {k: v for k, v in all_metrics.items() if v is not None}\n",
    "\n",
    "if not all_metrics:\n",
    "    print(\"\\n✗ No evaluation results available. Please check:\")\n",
    "    print(\"  1. Model checkpoint exists\")\n",
    "    print(\"  2. Test data files exist\")\n",
    "    print(\"  3. Evaluation ran without errors\")\n",
    "else:\n",
    "    print(f\"\\n✓ Successfully evaluated on {len(all_metrics)} dataset(s)\")\n",
    "    \n",
    "    # Save combined results\n",
    "    results_json = RESULTS_DIR / \"all_results.json\"\n",
    "    with open(results_json, 'w') as f:\n",
    "        json.dump(all_metrics, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics:\n",
    "    print(\"=\"*80)\n",
    "    print(\"OVERALL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for dataset_type, metrics in all_metrics.items():\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_type.replace('_', '-').title(),\n",
    "            'Accuracy (%)': metrics.get('valid_arithmetic_acc', 0),\n",
    "            'Perfect (%)': metrics.get('valid_arithmetic_perfect', 0),\n",
    "            'Correct (%)': metrics.get('valid_arithmetic_correct', 0),\n",
    "            'XE Loss': metrics.get('valid_arithmetic_xe_loss', 0)\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    summary_csv = RESULTS_DIR / \"overall_summary.csv\"\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(f\"\\nSummary saved to: {summary_csv}\")\n",
    "else:\n",
    "    print(\"No metrics to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_metrics(metrics, task):\n",
    "    \"\"\"\n",
    "    Extract per-class accuracy metrics.\n",
    "    \"\"\"\n",
    "    class_data = []\n",
    "    \n",
    "    for key, value in metrics.items():\n",
    "        if key.startswith('valid_arithmetic_acc_'):\n",
    "            class_id = key.split('_')[-1]\n",
    "            if class_id not in ['d1', 'd2', 'd3']:\n",
    "                class_data.append({\n",
    "                    'Class ID': class_id,\n",
    "                    'Class': interpret_class_id(class_id, task),\n",
    "                    'Accuracy (%)': value\n",
    "                })\n",
    "    \n",
    "    # Sort by class ID\n",
    "    class_data.sort(key=lambda x: int(x['Class ID']) if x['Class ID'].isdigit() else float('inf'))\n",
    "    return class_data\n",
    "\n",
    "\n",
    "if all_metrics:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PER-CLASS PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for dataset_type, metrics in all_metrics.items():\n",
    "        print(f\"\\n{dataset_type.upper().replace('_', '-')} DATASET:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        class_data = extract_class_metrics(metrics, TASK)\n",
    "        \n",
    "        if class_data:\n",
    "            class_df = pd.DataFrame(class_data)\n",
    "            display(class_df)\n",
    "            \n",
    "            # Save to CSV\n",
    "            class_csv = RESULTS_DIR / f\"class_performance_{dataset_type}.csv\"\n",
    "            class_df.to_csv(class_csv, index=False)\n",
    "            print(f\"Saved to: {class_csv}\")\n",
    "        else:\n",
    "            print(\"No per-class metrics found\")\n",
    "else:\n",
    "    print(\"No metrics to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics and len(all_metrics) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    ax = axes[0]\n",
    "    datasets = [k.replace('_', '-').title() for k in all_metrics.keys()]\n",
    "    accuracies = [v.get('valid_arithmetic_acc', 0) for v in all_metrics.values()]\n",
    "    colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "    \n",
    "    bars = ax.bar(datasets, accuracies, color=colors[:len(datasets)], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{acc:.2f}%',\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'Overall Accuracy Comparison\\n{ENCODING}, {TASK.upper()} Task', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 105])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: XE Loss comparison\n",
    "    ax = axes[1]\n",
    "    losses = [v.get('valid_arithmetic_xe_loss', 0) for v in all_metrics.values()]\n",
    "    \n",
    "    bars = ax.bar(datasets, losses, color=colors[:len(datasets)], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, loss in zip(bars, losses):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{loss:.4f}',\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax.set_ylabel('Cross-Entropy Loss', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'Cross-Entropy Loss Comparison\\n{ENCODING}, {TASK.upper()} Task', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = RESULTS_DIR / \"overall_comparison.png\"\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Figure saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No metrics to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Per-Class Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics:\n",
    "    # Collect per-class data for all datasets\n",
    "    class_comparison = {}\n",
    "    \n",
    "    for dataset_type, metrics in all_metrics.items():\n",
    "        for key, value in metrics.items():\n",
    "            if key.startswith('valid_arithmetic_acc_'):\n",
    "                class_id = key.split('_')[-1]\n",
    "                if class_id not in ['d1', 'd2', 'd3']:\n",
    "                    if class_id not in class_comparison:\n",
    "                        class_comparison[class_id] = {}\n",
    "                    class_comparison[class_id][dataset_type] = value\n",
    "    \n",
    "    if class_comparison:\n",
    "        # Create DataFrame for plotting\n",
    "        plot_data = []\n",
    "        for class_id, datasets in class_comparison.items():\n",
    "            for dataset_type, acc in datasets.items():\n",
    "                plot_data.append({\n",
    "                    'Class': interpret_class_id(class_id, TASK),\n",
    "                    'Dataset': dataset_type.replace('_', '-').title(),\n",
    "                    'Accuracy': acc\n",
    "                })\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Get unique classes and datasets\n",
    "        classes = plot_df['Class'].unique()\n",
    "        datasets = plot_df['Dataset'].unique()\n",
    "        \n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.25\n",
    "        colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "        \n",
    "        # Plot bars for each dataset\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            dataset_data = plot_df[plot_df['Dataset'] == dataset]\n",
    "            values = [dataset_data[dataset_data['Class'] == c]['Accuracy'].values[0] \n",
    "                     if len(dataset_data[dataset_data['Class'] == c]) > 0 else 0 \n",
    "                     for c in classes]\n",
    "            \n",
    "            offset = width * (i - len(datasets)/2 + 0.5)\n",
    "            bars = ax.bar(x + offset, values, width, label=dataset, \n",
    "                         color=colors[i % len(colors)], alpha=0.8, edgecolor='black')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, val in zip(bars, values):\n",
    "                if val > 0:\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{val:.1f}',\n",
    "                           ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Class', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'Per-Class Accuracy Comparison Across Datasets\\n{ENCODING}, {TASK.upper()} Task',\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(classes, rotation=15, ha='right')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.set_ylim([0, 105])\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = RESULTS_DIR / \"per_class_comparison.png\"\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Figure saved to: {fig_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No per-class metrics to visualize\")\n",
    "else:\n",
    "    print(\"No metrics to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL TEST PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nModel: {MODEL_DIR}\")\n",
    "    print(f\"Checkpoint: {CHECKPOINT_NAME}\")\n",
    "    print(f\"Encoding: {ENCODING}\")\n",
    "    print(f\"Task: {TASK.upper()}\")\n",
    "    print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nFiles generated:\")\n",
    "    for f in RESULTS_DIR.iterdir():\n",
    "        print(f\"  - {f.name}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"\\nNo results to summarize. Please check the error messages above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
