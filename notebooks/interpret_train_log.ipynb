{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Log Interpreter\n",
    "\n",
    "This notebook parses and visualizes training logs from the Int2Int model training.\n",
    "It extracts model parameters, task information, and plots metrics over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Specify the path to your training log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train log file path\n",
    "log_file_path = \"models/basic/1/train.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_log(log_path):\n",
    "    \"\"\"\n",
    "    Parse training log file to extract parameters and metrics.\n",
    "    \n",
    "    Returns:\n",
    "        params: dict of training parameters\n",
    "        metrics: list of dicts containing epoch metrics\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    metrics = []\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    # Parse parameters (they appear at the beginning of the log)\n",
    "    in_params_section = False\n",
    "    for line in lines:\n",
    "        # Detect start of parameters section\n",
    "        if 'Initialized logger' in line:\n",
    "            in_params_section = True\n",
    "            continue\n",
    "        \n",
    "        # Detect end of parameters section\n",
    "        if in_params_section and ('Running command' in line or 'Starting epoch' in line):\n",
    "            in_params_section = False\n",
    "        \n",
    "        # Parse parameter lines\n",
    "        if in_params_section:\n",
    "            # Parameters are in format: \"key: value\"\n",
    "            match = re.search(r'^\\s+(\\w+):\\s+(.+)$', line)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2).strip()\n",
    "                params[key] = value\n",
    "        \n",
    "        # Parse metric lines (contain __log__:)\n",
    "        if '__log__:' in line:\n",
    "            # Extract JSON part after __log__:\n",
    "            json_str = line.split('__log__:', 1)[1]\n",
    "            try:\n",
    "                metric_data = json.loads(json_str)\n",
    "                metrics.append(metric_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Could not parse line: {line[:100]}...\")\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    return params, metrics\n",
    "\n",
    "\n",
    "# Parse the log file\n",
    "params, metrics = parse_train_log(log_file_path)\n",
    "\n",
    "print(f\"Parsed {len(params)} parameters and {len(metrics)} epoch logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Model Configuration and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_task_type(params):\n",
    "    \"\"\"\n",
    "    Determine if this is mu or musq task based on training/eval data paths.\n",
    "    \"\"\"\n",
    "    train_data = params.get('train_data', '')\n",
    "    eval_data = params.get('eval_data', '')\n",
    "    \n",
    "    # Check for musq in the data paths\n",
    "    if 'musq' in train_data.lower() or 'musq' in eval_data.lower():\n",
    "        return 'musq'\n",
    "    elif 'mu' in train_data.lower() or 'mu' in eval_data.lower():\n",
    "        return 'mu'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "def print_model_config(params):\n",
    "    \"\"\"\n",
    "    Print key model configuration and training parameters.\n",
    "    \"\"\"\n",
    "    task_type = determine_task_type(params)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL CONFIGURATION AND TRAINING PARAMETERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüìã EXPERIMENT INFO\")\n",
    "    print(f\"  Experiment Name: {params.get('exp_name', 'N/A')}\")\n",
    "    print(f\"  Experiment ID: {params.get('exp_id', 'N/A')}\")\n",
    "    print(f\"  Task Type: {task_type.upper()}\")\n",
    "    print(f\"  Operation: {params.get('operation', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nüìä DATA\")\n",
    "    print(f\"  Training Data: {params.get('train_data', 'N/A')}\")\n",
    "    print(f\"  Eval Data: {params.get('eval_data', 'N/A')}\")\n",
    "    print(f\"  Data Types: {params.get('data_types', 'N/A')}\")\n",
    "    print(f\"  Base: {params.get('base', 'N/A')}\")\n",
    "    print(f\"  Modulus: {params.get('modulus', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nüèóÔ∏è MODEL ARCHITECTURE\")\n",
    "    print(f\"  Architecture: {params.get('architecture', 'N/A')}\")\n",
    "    print(f\"  Encoder Layers: {params.get('n_enc_layers', 'N/A')}\")\n",
    "    print(f\"  Decoder Layers: {params.get('n_dec_layers', 'N/A')}\")\n",
    "    print(f\"  Encoder Embedding Dim: {params.get('enc_emb_dim', 'N/A')}\")\n",
    "    print(f\"  Decoder Embedding Dim: {params.get('dec_emb_dim', 'N/A')}\")\n",
    "    print(f\"  Encoder Heads: {params.get('n_enc_heads', 'N/A')}\")\n",
    "    print(f\"  Decoder Heads: {params.get('n_dec_heads', 'N/A')}\")\n",
    "    print(f\"  Dropout: {params.get('dropout', 'N/A')}\")\n",
    "    print(f\"  Attention Dropout: {params.get('attention_dropout', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è TRAINING PARAMETERS\")\n",
    "    print(f\"  Optimizer: {params.get('optimizer', 'N/A')}\")\n",
    "    print(f\"  Batch Size: {params.get('batch_size', 'N/A')}\")\n",
    "    print(f\"  Eval Batch Size: {params.get('batch_size_eval', 'N/A')}\")\n",
    "    print(f\"  Epoch Size: {params.get('epoch_size', 'N/A')}\")\n",
    "    print(f\"  Max Epochs: {params.get('max_epoch', 'N/A')}\")\n",
    "    print(f\"  Eval Size: {params.get('eval_size', 'N/A')}\")\n",
    "    print(f\"  Gradient Clipping: {params.get('clip_grad_norm', 'N/A')}\")\n",
    "    print(f\"  Max Length: {params.get('max_len', 'N/A')}\")\n",
    "    print(f\"  Max Output Length: {params.get('max_output_len', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nüîß OTHER SETTINGS\")\n",
    "    print(f\"  FP16: {params.get('fp16', 'N/A')}\")\n",
    "    print(f\"  CPU Mode: {params.get('cpu', 'N/A')}\")\n",
    "    print(f\"  Multi-GPU: {params.get('multi_gpu', 'N/A')}\")\n",
    "    print(f\"  Num Workers: {params.get('num_workers', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return task_type\n",
    "\n",
    "\n",
    "task_type = print_model_config(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Metrics to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics to DataFrame for easier analysis\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"Total epochs recorded: {len(df_metrics)}\")\n",
    "print(\"\\nFirst few epochs:\")\n",
    "df_metrics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if 'valid_arithmetic_xe_loss' in df_metrics.columns:\n",
    "    plt.plot(df_metrics['epoch'], df_metrics['valid_arithmetic_xe_loss'], \n",
    "             marker='o', linewidth=2, markersize=4, label='XE Loss')\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.title(f'Validation Cross-Entropy Loss Over Epochs\\n({task_type.upper()} task)', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # Add min value annotation\n",
    "    min_loss = df_metrics['valid_arithmetic_xe_loss'].min()\n",
    "    min_epoch = df_metrics.loc[df_metrics['valid_arithmetic_xe_loss'].idxmin(), 'epoch']\n",
    "    plt.axhline(y=min_loss, color='r', linestyle='--', alpha=0.5, label=f'Min: {min_loss:.4f} (epoch {min_epoch})')\n",
    "    plt.legend(fontsize=10)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'valid_arithmetic_xe_loss not found in metrics', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Accuracy Metrics (acc, perfect, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "acc_metrics = ['valid_arithmetic_acc', 'valid_arithmetic_perfect', 'valid_arithmetic_correct']\n",
    "colors = ['blue', 'green', 'orange']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for metric, color, marker in zip(acc_metrics, colors, markers):\n",
    "    if metric in df_metrics.columns:\n",
    "        plt.plot(df_metrics['epoch'], df_metrics[metric], \n",
    "                marker=marker, linewidth=2, markersize=4, \n",
    "                color=color, label=metric.replace('valid_arithmetic_', ''))\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title(f'Validation Accuracy Metrics Over Epochs\\n({task_type.upper()} task)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.ylim([0, 105])  # Set y-axis range for percentages\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Class-specific Accuracies (acc_0, acc_1, acc_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "class_metrics = ['valid_arithmetic_acc_0', 'valid_arithmetic_acc_1', 'valid_arithmetic_acc_100']\n",
    "colors = ['purple', 'red', 'cyan']\n",
    "markers = ['D', 'v', 'p']\n",
    "labels = ['Class 0 (Œº=0)', 'Class 1 (Œº=1)', 'Class 100 (Œº=-1)']\n",
    "\n",
    "has_any_metric = False\n",
    "for metric, color, marker, label in zip(class_metrics, colors, markers, labels):\n",
    "    if metric in df_metrics.columns:\n",
    "        has_any_metric = True\n",
    "        plt.plot(df_metrics['epoch'], df_metrics[metric], \n",
    "                marker=marker, linewidth=2, markersize=4, \n",
    "                color=color, label=label)\n",
    "\n",
    "if has_any_metric:\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    \n",
    "    if task_type == 'mu':\n",
    "        plt.title(f'Class-Specific Accuracies Over Epochs (MU task)\\nClass 0: Œº=0, Class 1: Œº=1, Class 100: Œº=-1', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        plt.title(f'Class-Specific Accuracies Over Epochs ({task_type.upper()} task)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.ylim([0, 105])  # Set y-axis range for percentages\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No class-specific accuracy metrics found', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: Digit-wise Accuracies (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for digit-wise accuracy metrics\n",
    "digit_metrics = [col for col in df_metrics.columns if 'acc_d' in col]\n",
    "\n",
    "if digit_metrics:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for metric in digit_metrics:\n",
    "        plt.plot(df_metrics['epoch'], df_metrics[metric], \n",
    "                marker='o', linewidth=2, markersize=4, \n",
    "                label=metric.replace('valid_arithmetic_', ''))\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title(f'Digit-wise Accuracies Over Epochs\\n({task_type.upper()} task)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.ylim([0, 105])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No digit-wise accuracy metrics found in the log.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Metrics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_best_metrics(df_metrics):\n",
    "    \"\"\"\n",
    "    Report the best (minimum for loss, maximum for accuracies) metrics across all epochs.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BEST METRICS ACROSS ALL EPOCHS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Cross-Entropy Loss (lower is better)\n",
    "    if 'valid_arithmetic_xe_loss' in df_metrics.columns:\n",
    "        min_loss = df_metrics['valid_arithmetic_xe_loss'].min()\n",
    "        min_loss_epoch = df_metrics.loc[df_metrics['valid_arithmetic_xe_loss'].idxmin(), 'epoch']\n",
    "        print(\"\\nüìâ CROSS-ENTROPY LOSS (lower is better)\")\n",
    "        print(f\"  Best Loss: {min_loss:.6f}\")\n",
    "        print(f\"  Achieved at Epoch: {min_loss_epoch}\")\n",
    "    \n",
    "    # Accuracy metrics (higher is better)\n",
    "    print(\"\\nüìà ACCURACY METRICS (higher is better)\")\n",
    "    acc_metrics = ['valid_arithmetic_acc', 'valid_arithmetic_perfect', 'valid_arithmetic_correct']\n",
    "    \n",
    "    for metric in acc_metrics:\n",
    "        if metric in df_metrics.columns:\n",
    "            max_acc = df_metrics[metric].max()\n",
    "            max_acc_epoch = df_metrics.loc[df_metrics[metric].idxmax(), 'epoch']\n",
    "            metric_name = metric.replace('valid_arithmetic_', '').upper()\n",
    "            print(f\"\\n  {metric_name}:\")\n",
    "            print(f\"    Best: {max_acc:.2f}%\")\n",
    "            print(f\"    Achieved at Epoch: {max_acc_epoch}\")\n",
    "    \n",
    "    # Class-specific accuracies\n",
    "    class_metrics = ['valid_arithmetic_acc_0', 'valid_arithmetic_acc_1', 'valid_arithmetic_acc_100']\n",
    "    class_labels = ['Class 0 (Œº=0)', 'Class 1 (Œº=1)', 'Class 100 (Œº=-1)']\n",
    "    \n",
    "    has_class_metrics = any(metric in df_metrics.columns for metric in class_metrics)\n",
    "    \n",
    "    if has_class_metrics:\n",
    "        print(\"\\nüìä CLASS-SPECIFIC ACCURACIES (higher is better)\")\n",
    "        for metric, label in zip(class_metrics, class_labels):\n",
    "            if metric in df_metrics.columns:\n",
    "                max_acc = df_metrics[metric].max()\n",
    "                max_acc_epoch = df_metrics.loc[df_metrics[metric].idxmax(), 'epoch']\n",
    "                print(f\"\\n  {label}:\")\n",
    "                print(f\"    Best: {max_acc:.2f}%\")\n",
    "                print(f\"    Achieved at Epoch: {max_acc_epoch}\")\n",
    "    \n",
    "    # Digit-wise accuracies\n",
    "    digit_metrics = [col for col in df_metrics.columns if 'acc_d' in col]\n",
    "    \n",
    "    if digit_metrics:\n",
    "        print(\"\\nüî¢ DIGIT-WISE ACCURACIES (higher is better)\")\n",
    "        for metric in sorted(digit_metrics):\n",
    "            max_acc = df_metrics[metric].max()\n",
    "            max_acc_epoch = df_metrics.loc[df_metrics[metric].idxmax(), 'epoch']\n",
    "            metric_name = metric.replace('valid_arithmetic_', '').upper()\n",
    "            print(f\"\\n  {metric_name}:\")\n",
    "            print(f\"    Best: {max_acc:.2f}%\")\n",
    "            print(f\"    Achieved at Epoch: {max_acc_epoch}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total Epochs Recorded: {len(df_metrics)}\")\n",
    "    if 'epoch' in df_metrics.columns:\n",
    "        print(f\"Epoch Range: {df_metrics['epoch'].min()} - {df_metrics['epoch'].max()}\")\n",
    "    \n",
    "    # Latest epoch metrics\n",
    "    if len(df_metrics) > 0:\n",
    "        print(\"\\nüìç LATEST EPOCH METRICS:\")\n",
    "        last_row = df_metrics.iloc[-1]\n",
    "        print(f\"  Epoch: {last_row['epoch']}\")\n",
    "        if 'valid_arithmetic_xe_loss' in last_row:\n",
    "            print(f\"  XE Loss: {last_row['valid_arithmetic_xe_loss']:.6f}\")\n",
    "        if 'valid_arithmetic_acc' in last_row:\n",
    "            print(f\"  Accuracy: {last_row['valid_arithmetic_acc']:.2f}%\")\n",
    "        if 'valid_arithmetic_perfect' in last_row:\n",
    "            print(f\"  Perfect: {last_row['valid_arithmetic_perfect']:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "report_best_metrics(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Metrics to CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export metrics to CSV\n",
    "# output_csv = log_file_path.replace('.log', '_metrics.csv')\n",
    "# df_metrics.to_csv(output_csv, index=False)\n",
    "# print(f\"Metrics exported to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all metrics for inspection\n",
    "print(\"\\nAll available metric columns:\")\n",
    "print(df_metrics.columns.tolist())\n",
    "\n",
    "print(\"\\nFull metrics table:\")\n",
    "df_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
